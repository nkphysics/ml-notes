{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5007560a-bdf8-416f-a055-d17e3630f4ff",
   "metadata": {},
   "source": [
    "# OneR Algorithm for Classification\n",
    "\n",
    "## One Sentence Summary\n",
    "\n",
    "The OneR algorithm for classification is one of the simplest rule-based classifiers, selecting the feature with the lowest classification error and assigning the most frequent class within each feature value to make predictions.\n",
    "\n",
    "## General Summary\n",
    "\n",
    "The OneR classifier is one of the simplest machine learning algorithms for classification with categorical data. It works by evaluating each feature using contingency tables to find the feature that separates classes with the least error. Once the best feature is identified, the majority class in each group of the best feature is used as the rule for classifying. This makes it ideal for situations where interpretability is important, but it may sacrifice some accuracy compared to more complex models. Its appeal lies in its ability to quickly highlight the most important feature in a dataset.\n",
    "\n",
    "## Full Discussion\n",
    "\n",
    "The OneR classifier (short for one rule) is a simple yet effective rule-based algorithm designed for classification tasks, particularly when working with **categorical data**. At its core, OneR operates on the principle that a single feature can often provide sufficient information to make accurate predictions, provided that feature is chosen carefully. This simplicity is what makes OneR a popular choice for scenarios where interpretability, transparency, and ease of explainability are prioritized over complex modeling. By relying on a single, well-chosen rule, OneR ensures that its decisions are easy to understand in any application where model explainability is essential.\n",
    "\n",
    "The algorithm works in two main phases: feature selection and rule creation. During the fitting phase, OneR evaluates each feature in the dataset by constructing **contingency tables**. This is a process that quantifies how well each feature separates the classes. For each feature, the algorithm groups the data by the feature's unique values and calculates the **majority class** within each group. It then determines how many misclassifications would occur if that feature were used to make predictions. The feature with the lowest qunatity of errors is selected as the best feature, and the corresponding rules are established. This process ensures that the model is not only simple but also empirically grounded, as it explicitly minimizes the risk of incorrect predictions based on the data itself.\n",
    "\n",
    "Once the best feature is identified, the prediction phase becomes straightforward: for any new input, the model simply looks up the value of the best feature and applies the corresponding rule to assign a class label. This approach eliminates the need for complex calculations or probabilistic reasoning, making OneR one of the most computationally efficient classifiers available. However, this simplicity comes with a trade-off: while OneR excels in interpretability and ease of use, it may sacrifice accuracy compared to more sophisticated models that leverage multiple features or nonlinear relationships. Despite this limitation, OneR remains a valuable tool for exploratory analysis, feature selection, and scenarios where a quick, interpretable baseline is needed before deploying more complex models.\n",
    "\n",
    "The following implementation of OneR has been created to operate like a sklearn classifier, with methods such as `fit`, `predict`, and `score` that align with the standard machine learning workflow.\n",
    "\n",
    "Following this implementation of `OneR` we will apply it to a subset of the popular machine learning *Titanic* toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ce048141-f70a-43e1-922f-9e3a9e03d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class OneR:\n",
    "    def __init__(self):\n",
    "        self.best_feature = None\n",
    "        self.rule = {}\n",
    "        self.classes = None\n",
    "        self.ftables = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the OneR model according to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Feature matrix (training data).\n",
    "        y : pd.Series or np.ndarray\n",
    "            Target vector (class labels).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        if not isinstance(y, pd.Series):\n",
    "            y = pd.Series(y)\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "\n",
    "        best_error = float('inf')\n",
    "        best_rule = {}\n",
    "        best_feature = None\n",
    "\n",
    "        for feature in X.columns:\n",
    "            # Create a temporary DataFrame with the feature and target\n",
    "            temp = pd.concat([X[feature], y], axis=1)\n",
    "            grouped = temp.groupby(feature)\n",
    "\n",
    "            rule = {}\n",
    "            total_error = 0\n",
    "\n",
    "            for name, group in grouped:\n",
    "                # Get the majority class in this group\n",
    "                majority_class = group.iloc[:, 1].mode()[0]\n",
    "                rule[name] = majority_class\n",
    "                maj_total = (group.iloc[:, 1] == majority_class).sum()\n",
    "                # Count misclassifications\n",
    "                total_error += len(group) - maj_total\n",
    "\n",
    "            # Add frequency table to ftables\n",
    "            freq_table = grouped[y.name].value_counts().unstack(fill_value=0)\n",
    "            self.ftables[feature] = freq_table\n",
    "\n",
    "            # Check if this feature is better\n",
    "            if total_error < best_error:\n",
    "                best_error = total_error\n",
    "                best_rule = rule\n",
    "                best_feature = feature\n",
    "\n",
    "        self.best_feature = best_feature\n",
    "        self.rule = best_rule\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame or np.ndarray\n",
    "            Feature matrix (test data).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : np.ndarray\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        if self.best_feature is None:\n",
    "            raise ValueError(\"Model not fitted yet. Call 'fit' first.\")\n",
    "\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for _, row in X.iterrows():\n",
    "            feature_value = row[self.best_feature]\n",
    "            # Use the best rule to predict the class\n",
    "            pred = self.rule.get(feature_value, 0)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame or np.ndarray\n",
    "            Feature matrix (test data).\n",
    "        y : pd.Series or np.ndarray\n",
    "            True labels for X.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        accuracy : float\n",
    "            Mean accuracy of the classifier.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19fd52-9fec-430b-ba80-292f82d34f4e",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The following code demonstrates how the OneR classifier can be applied to a subset of categorical features from the popular machine learning Titanic dataset. The dataset contains information about passengers aboard the Titanic, including the following categorical features we'll use with OneR:\n",
    "\n",
    " - Sex\n",
    "    - Female\n",
    "    - Male\n",
    " - Pclass (passenger class)\n",
    "    - 1 (1st class, most expensive)\n",
    "    - 2 (2nd class)\n",
    "    - 3 (3rd class, cheapest)\n",
    " - Embarked (port of embarkation)\n",
    "    - C (Cherbourg)\n",
    "    - Q (Queenstown)\n",
    "    - S (Southampton)\n",
    "\n",
    "These features are set to `X` in the following code. The target column \"Survived\" is set to `y` and is either 1 (Survived the titanic) or 0 if then did not.\n",
    "\n",
    "As per standard practice we are splitting the data with `train_test_split` from `sklearn`. Then, we create a `OneR` object, fit with the testing data, and finally predict and score from the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3efb215c-13d4-4748-892c-cb2ee1bd2c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.785\n",
      "\n",
      "Best Feature: Rule\n",
      "Sex: {'female': 1, 'male': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"datasets/titanic/train.csv\")\n",
    "\n",
    "X = df[[\"Sex\", \"Pclass\", \"Embarked\"]]\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "model = OneR()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\\n\")\n",
    "print(f\"Best Feature: Rule\\n{model.best_feature}: {model.rule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4723ac7f-a0a1-4a62-b635-f5bb104068c1",
   "metadata": {},
   "source": [
    "From the testing and training with `OneR` we can see that the best feature for classification is *Sex* and the rule is that females will survive and males will not. This rule is accurate 78.5% of the time from the testing.\n",
    "\n",
    "To better understand this let's examine the contingency tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2b3ff105-7637-4a82-80e6-b8138ac3515f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived    0    1\n",
      "Sex               \n",
      "female     59  170\n",
      "male      356   83\n",
      "\n",
      "Survived    0   1\n",
      "Pclass           \n",
      "1          59  93\n",
      "2          73  68\n",
      "3         283  92\n",
      "\n",
      "Survived    0    1\n",
      "Embarked          \n",
      "C          52   65\n",
      "Q          37   21\n",
      "S         326  165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in model.ftables.keys():\n",
    "    print(f\"{model.ftables[feature]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4a334-97c0-4c6d-b8a4-c38ab878c89e",
   "metadata": {},
   "source": [
    "The first table for the *Sex* feature shows 170 females survived, while 356 males did not survive, and as stated previously this was identified as the best feature of the dataset. The rule was therefore established as females survive (classify as 1), males do not survive (classify as 0). With this rule established we can see that 356 males and 170 females would be correctly classified, leaving $59+83=142$ misclassifications.\n",
    "\n",
    "If we follow the same logic and use the mode (majority) of each row of the other repective tables to establish rules for both features we would get that for *Pclass* only class 1 passengers would be classified as survivors, while for the *Embarked* feature only passengers that embarked from Cherbourg (C) would be classified as survivors. That would leave $59+68+92=219$ misclassifications for *Pclass*, and $52+21+165=238$ misclassifications for *Embarked*. Since the *Sex* feature had the fewest misclassifications it was selected as the best feature of the dataset. Testing `OneR` on the testing data gave the score of $0.785$, so we would expect for this to be the best possible accuracy of all of these features.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "OneRâ€™s strengths in interpretability and speed make it a practical choice for quick prototyping or initial data exploration. While it may not match the predictive accuracy of more complex classifiers, its simplicity and computational efficiency make it an excellent choice for exploratory analysis or just a baseline benchmarking. By balancing simplicity with utility, OneR highlights the value of pragmatic approaches when classifying. Sometimes the best solution can the simplest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f919f68-54c1-4b29-9baf-95d8e2266a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
